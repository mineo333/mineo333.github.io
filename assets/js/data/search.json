[ { "title": "A Look At mmap - Part 1", "url": "/posts/A-Look-At-mmap-Part-1/", "categories": "QogChamp", "tags": "Linux Kernel Exploitation, Syscall", "date": "2022-04-30 14:00:00 +0000", "snippet": "DisclaimerAs always, nothing in this article is guaranteed to be accurate nor correct.If you see anything blatantly wrong or missing from these articles, do not hesitate to email me at sharad@mineo333.dev or dm me on discord at write(1,&quot;mineo333&quot;,8);#3385IntroductionIn these next few articles, we’re going to discuss exactly how PAGECACHE_TAG_DIRTY is set. We will start by discussing the mmap syscall.For brevity purposes, this article is split into a few subparts, as one singular article would be too much.Intrommap is perhaps one of the most important syscalls in the Linux operating system, as it is how memory mappings are formed. It is through mmap that programs are run, shared libraries are linked, and so much more. In other words, it is safe to say that without mmap, Unix operating systems would crumble.Memory MappingsOne thing we need to thoroughly understand before we even attempt to understand mmap is memory mappings.A memory mapping can simply be described as a page-aligned, reserved area in the virtual address space. What the purpose of this reserved area is depends on the memory mapping.There are exactly 4 types of memory mappings: private/file-mapped, shared/file-mapped, private/anonymous, and shared/anonymous.File-Backed vs AnonymousLet’s first discuss the difference between file-mapped and anonymous mappings.Put simply, a file-backed mapping is a mapping whose data comes from a file.For example, suppose we have a file-backed mapping that maps libc.so.6 and exists between virtual addresses 0x1000 and 0x3000. In this case, if you dereferenced the address 0x1000, you would get the first byte of libc.so.6. Likewise, if you dereference 0x2000, you would get the byte 4096th byte in libc.so.6In this sense, the file is literally mapped to memory.Note that certain limitations do exist for file-backed mappings. There are two limitations.The first limitation essentially forces you to map the file one page at a time. So, you must map 4096 bytes of the file at a time. As a result, you can’t map only the first 500 bytes. Instead, you must map the first 4096 bytes. In addition, the mappings must be on page boundaries within the file. So, you can’t map bytes 500 - 4596. Instead, you must map bytes 0 - 4096, 4096 - 8192, etc.The second limitation states that the mappings must be contiguous within the file. So, you can’t map bytes 4096 - 8192 and 12288 - 16384 in the same memory mapping. Instead, you must map the entire range of 4096 - 16384 or put them in 2 different memory mappings.File-backed mappings are most often used to map certain portions of an executable when running a program.The second kind of memory mapping is an anonymous mapping. Anonymous mappings are exactly the opposite of file-backed mappings - they are not file backed. As such, instead of being filled with file-data, anonymous mappings are initialized to 0 bytes. Common examples of anonymous mappings include the heap and stack, as they are never backed by any particular file.Shared vs PrivateWhile file-backed vs anonymous has to do with the actual contents of the mappings, shared vs private has to do with how mappings interact with multiple processes.Shared vs private can often be slightly more confusing than file-backed vs anonymous, as the meaning can change depending on if we are dealing with a file-backed or private mapping.Let’s start by discussing shared mappings.Shared mappings are mappings that are shared between processes and the disk. What this means depends on if it is anonymous or file-backed.If a shared mapping is anonymous, then, if fork occurs, the resulting process will have the same mapping pointing to the exact same pages.For example, suppose Process A has a shared anonymous mapping between addresses 0xf000 and 0x10000. Suppose Process A does a fork and creates Process B. If Process B writes to any address between 0xf000 and 0x10000, Process A will see those changes. Because of this behavior, shared anonymous mappings are very rarely used aside from certain IPC implementations.If a shared mapping is file-backed, then any writes made to the mapping will be written back to the disk.For example, suppose Process A maps a file called example.txt to memory between addresses 0x1000 and 0x3000. Because the mapping is shared and file-backed, any changes made to any address in that range will be seen in example.txt.Naturally, if a process with a shared file-mapping does a fork, any changes made to the mapping can be seen by both the parent and the child (As the changes are committed to the disk).On the other hand, private mappings are mappings that are private to a particular process. Another term that is used for these mappings is copy-on-write (CoW) as the pages in these mappings are copy-on-write. Like with shared mappings, the meaning of this changes depending on whether the mapping is file-backed or anonymous.If a private mapping is anonymous, then it is copy-on-write between processes.For example, suppose Process A has a private anonymous mapping between 0x1000 and 0x3000. Suppose Process A then does a fork creating Process B. If Process B writes to any byte between 0x1000 and 0x3000, then Process A will not see that change.We will discuss how exactly the kernel accomplishes this when we talk about page faults and the mmap implementation.If a private mapping is file-backed, then any write will not be committed to the disk. Similarly, much like with anonymous mappings, any writes made to file-backed mappings will also not be shared by processes. In other words, file-backed, private mappings are anonymous mappings + the changes don’t get committed to the disk.With an understanding of all of this, we are prepared to talk about the mmap syscall." }, { "title": "sync and Writeback", "url": "/posts/4-sync-and-writeback/", "categories": "QogChamp", "tags": "Linux Kernel Exploitation, Syscall", "date": "2021-11-20 22:40:00 +0000", "snippet": "In this article, I want to describe the sync syscall. By describing the sync, I hope to describe how changes to files are transferred from the page cache to the disk. This will give critical insight into how Qogchamp works.How to read this articleThis article goes step by step through the msync and fsync syscalls. I HIGHLY recommend that you follow along on your own as I don’t put much of the actual code in this article. In order to follow along either start at the entrypoints mentioned, or, if you’re skipping around, use this wonderful website to search look up the symbols.EntrypointIn order to begin analyzing sync, it is important to find the entrypoint. Luckily, we can use this wonderful site that describes all the entrypoints for each syscall and their registers.Looking at this site, we find three kinds of sync - msync, fsync, and plain sync. For ease, we will analyze both msync and fsync. We won’t analyze sync because it wakes up the flusher threads, which makes it somewhat difficult to analyze.msync vs. fsyncI briefly want to go over the difference between msync and fsync at a high level before we get into the specifics.There are two ways to open and modify files for those unaware: write and mmap.In the case of write, we simply open the file using open, and then we use write along with the returned file descriptor. In the case of write, as we will see, the changes are written directly to the backing struct address_space of the inode. In order manually to force the contents of the written file back to the disk, we use fsync, which takes in the file descriptor received from open.In the case of mmap, we map the file into the process’s address space. Meaning, we can access and modify portions of the file in the same way we might access portions of stack or heap. These modifications are then written back to the page cache. msync is used to force back those changes to the disk.fsyncUsing the site mentioned above, we can find the entry point of fsync, which happens to be in fs/sync.c which can be found here. In order to find the actual syscall within the file, we can look for the SYSCALL_DEFINEx macro.Quickly, we find that we enter a function called do_fsync with the fd and another argument datasync being 0. datasync is only set to 1 if fdatasync is called.Within do_fsync we call fdget which returns a struct fd. Those familiar with Linux file operations might recognize this function. Essentially, within the process control block of Linux - struct task_struct there is an array of struct fds containing information on opened files. Using this integer as an index into this array, we get the relevant struct fd. If you’re particularly interested in these functions, I recommend reading this.Using this struct fd, we enter another function known as vfs_fsync. Anyone familiar with Linux file operations might be familiar with the vfs_ prefix. This is derived the idea of the virtual file system. In the Linux Kernel, we have a generic high-level file-system which functions as the abstraction above concrete file systems (i.e. ext4). As we’ll see, these vfs_ functions typically eventually call file-systems-specific functions.In vfs_fsync we call vfs_fsync_range which is used to sync portions of a file. We use 0 and LLONG_MAX as the start and end, respectively, to sync the entire file. From here, we move into file-system-specific operations.In vfs_fsync_range, we begin with two if-statements. The first if-statement checks if the file system has a fsync member function. If not, then it errors. This happens because the virtual file system needs a concrete fsync function provided by the file-system in order to perform fsync.The second if-statement is a little more peculiar. The second if-statement is where datasync becomes useful. If datasync is 0 (Which it is), then we essentially update the inode metadata and write that new metadata to the disk. If datasync is 1, then we do not update the metadata, and we only sync the contents of the file.When those two checks are complete, we finally call the file-system-specific fsync.Unfortunately, because of how many file-systems there are, I will not be able to cover the fsync implementation for every single one. So, I will only focus on ext4’s implementation, which can be found here. Looking at the fsync field, we find that it points to ext4_sync_file. This is what is called by vfs_fsync_range.File-system-Specific fsyncIn ext4_sync_file, we start out by checking if the superblock of the file-system is read only. If it is then we leave (goto out). This is fairly self-explanatory as we can’t write to a read-only file-system.After that check is completed, we immediately jump into file_write_and_wait_range, which actually happens to be a VFS function. In file_write_and_wait_range, we come across a familiar data structure: address_space, which, as you might recall, is the page cache data structure. As might be obvious, we are going to write the pages from address_space.The first thing that we check in file_write_and_wait_range is if the page-cache has any pages at all. This is done by running the function mapping_needs_writeback. This is an important check because if the page cache has no pages, then what’s the point of starting writeback?If the page cache for the inode does indeed have pages, we run __filemap_fdatawrite_range. __filemap_fdatawrite_range is run with four parameters. The first three are self-explanatory. The last one is interesting, however: WB_SYNC_ALL. What this flag does is that if a page in the page cache is already under writeback, then it should wait for the writeback to finish and then redo the writeback. The alternative to this is WB_SYNC_NONE, which does not wait for writeback to complete and instead skips any page currently under writeback. This distinction is best described in the documentation for __filemap_fdatawrite_range, which says that “The difference between [ WB_SYNC_NONE and WB_SYNC_ALL ] is that if a dirty page/buffer is encountered, it must be waited upon, and not just skipped over.” We will see these flags in the code down the road.Looking into __filemap_fdatawrite_range we see that we setup something called a writeback_control. This struct essentially decides how writeback will be done. There are 4 fields populated, but one that is new: nr_to_write. This represents the number of pages to writeback. This is set to LONG_MAX, indicating that we should writeback all pages (Which makes sense). Once that is set up, we jump into do_writepages.Within do_writepages, we arrive a fork in the road. If the file-system has an implementation for writepages in its address_space_operations then use that. If not, then use a generic_writepages. Looking into ext4’s address_space_operations, we see that ext4 has an implementation for writepages in ext4_writepages. So, we jump to that function.writepagesUpon reading ext4_writepages, we see a daunting block of code. However, upon closer analysis, it becomes obvious that much of this code is for dealing with special cases of writeback - specifically three cases: cyclic writeback, inline file writeback, and non-journaled writeback. However, for ease, we will assume that we take the standard writeback procedure which uses journalingAssuming we are performing journaling, we start at line 2671. Here, we first check if the file is being journaled. If it is (Which we are assuming), then we jump to generic_writepages.Within generic_writepages, we start by looking checking if the file-system’s address_space_operations has a writepage implementation. If it doesn’t then leave. This writepage function is what actually writes the new file buffer to the disk. So, without a writepage it is physically impossible to push data to the disk. If writepage exists, we then run a function called blk_start_plug. According to the documentation, all this does is warm up the block device and block io layer and warn them that they will be receiving IO requests shortly. From here, we jump to write_cache_pages. However, before we go into write_cache_pages, we need to analyze one of the parameters - specifically __writepage.write_cache_pages requires a parameter that is of type writepage_t. This is a pointer to a writepage function. In this case, we use __writepage which essentially serves as a wrapper of the FS-specific implementation of writepage.Looking into write_cache_pages we find a very large block of code. However, once again, much of this code is dealing with edge cases which we don’t particularly care about. The first real line of consequence (And perhaps the most important line in the entire fsync implementation) happens on line 2197. This is the line where the magic of QogChamp is defined. Assuming that the sync mode is WB_SYNC_ALL (Which it is), then we will jump into tag_pages_for_writeback.In tag_pages_for_writeback, the kernel essentially searches through the page cache and tags all the dirty pages with a special tag that says that it needs to be written back. How does it do this? The answer hearkens back to the i_pages xarray structure within the address_space object. What it does is that it uses a special macro to find all entires that are marked with PAGECACHE_TAG_DIRTY. Every entry marked with this tag is then marked with PAGECACHE_TAG_TOWRITE, indicating that it should be written to the disk.There are two questions posed from this. First, why do this? The main reason for doing this is written in the documentation for tag_pages_for_writeback. We do this additional marking in order to prevent livelocking, which could be caused by a process writing to the pages as we’re trying to do the writeback. Second, where does the PAGECACHE_TAG_DIRTY tag come from, and when is it set? This is the answer to this question that holds the key to why QogChamp works. This topic will be discussed further as we discuss the mmap and write implementations.Once tag_pages_for_writeback returns, it can be assumed that all the pages ready for writeback are tagged and ready to go. From here, it’s just a matter of iterating through the pages and sending them on their way to the disk.In order to iterate through the pages, we create a pagevec. This pagevec is then populated with the pages that we need to write back to the disk via pagevec_lookup_range_tag. With this, we have everything we need to writeback.To writeback, the code does three checks: It checks if the page has been truncated under us. If so, then leave because a truncated page cannot be written back. This is accomplished by checking if the address_space associated with the page is still valid. It checks if the page is no longer dirty. If the page is no longer dirty, then the page has been written back by someone else. It checks if the page is already currently under writeback. If it is, then it waits for it to not be under writeback and then either continues or redoes the write depending on whether WB_SYNC_ALL or WB_SYNC_NONE is used. Once these three checks are satisfied, the dirty flag is cleared via clear_page_dirty_for_io, and then finally, we call writepage function that was passed in earlier (__writepage). As I said earlier, __writepage serves as a wrapper for the FS-specific writepage which in the case of ext4 is ext4_writepage.I’m not going to go over ext4_writepage, but essentially what it does is that it passes the page down the block device driver, which then pushes the page to the disk via DMA. This completes the journey of the average fsync.msyncLuckily, the msync implementation is not too different from the fsync.Much like with fsync, we at the entry for msync which can be found here.msync starts out by performing some checks. There are a variety of checks here. We first check the flags and if any flags that are not MS_ASYNC, MS_SYNC, or MS_INVALIDATE are set then return immediately. Second we check to see if both MS_ASYNC and MS_SYNC are set. If they are then return. Finally, we check if the inputted address and offset are valid.If these basic checks pass then we can start msync. msync starts out with find_vma which finds the vm_area_struct that contains the start. Now find_vma does the search in a particularly interesting (And quite perplexing) way. Within each vm_area_struct there are fields called vm_start and vm_end. vm_start represents the beginning virtual address of the vma and vm_end represents the ending virtual address of the vma. Now instead of checking that vm_start &amp;lt;= start &amp;lt; vm_end, it instead checks that start &amp;lt; vm_end and returns the first vma that satisfies that condition. So, we need to do some additional checking within msync. Specifically, we make sure that start &amp;gt;= vm_start.If all these checks are satisfied, then we can actually begin. We start by calculating the portion of the file that we want to sync which is then stored in fstart and fend. With these in hand we perform yet another which checks if 1. we have a file, 2. we are syncing synchronously, and 3. the mapping is shared. If these three conditions are satisfied then we pass fstart and fend into vfs_fsync_range and we continue with the same fsync routine. Note that in the case of fsync we passed in 0 and LLONG_MAX, but this time we are passing in only fstart and fend as we are syncing only the portion of the file specified by the user.Additional commentsAs you might have realized, Linux employs a very modular system when it comes to implementation. As we talked about fsync we were interleaving between different “layers” of the kernel. Initially, we start out at the virtual file system (vfs) level which called the file-system-specific function which then called the block-device-specific functions.This layering idea is something that is common across the entire linux kernel whether it be networking, file-systems, or anything else and is something that one should get used to.What’s NextFrom here, we are going to discuss how PAGECACHE_TAG_DIRTY is set by discussing mmap, write, and page faulting.Referencesext4’s address_space_operations - https://elixir.bootlin.com/linux/v5.14/source/fs/ext4/inode.c#L3648find_vma - https://elixir.bootlin.com/linux/v5.14/source/mm/mmap.c#L2301" }, { "title": "Linux Kernel Page Cache", "url": "/posts/3-linux-kernel-page-cache/", "categories": "QogChamp", "tags": "Linux Kernel Exploitation, Page Cache", "date": "2021-11-08 15:40:00 +0000", "snippet": "One of QogChamp’s capabilities is performing “ghost file operations” or file operations that are easy to clean up and have little to no footprint. The page cache is the main structure used to pull this off.What is a Page Cache?As you might know, block devices (Disks/SSDs) are EXTREMELY slow compared to RAM and CPU.Now, this is a big problem, especially for read/write operations, as every time a program wants to read/write to/from a file, it needs to wait for the disk to complete the operation. This, due to the slow nature of the block devices, can burn CPU cycles and force programs to block for extended periods while waiting for the disk to complete. In the long run, this is bad and can greatly extend the runtime of certain programs that perform file operations (So basically, almost every program).So, how do we solve this problem? As it turns out, the solution is surprisingly simple: make a copy of the file in memory. This makes file operations significantly faster as instead of constantly asking the disk, we simply perform our file operations (read, write, mmap, etc.) on memory which is much faster. We call this in-memory representation the page cache.A common question that might follow from this is if we’re doing all our file operations on the page cache, how do changes get synced with the disk?To sync changes, the Linux Kernel undergoes a process known as deferred write. With deferred write, the kernel waits until a good time to actually sync the changes with the disk. This is much faster than writing back immediately for one main reason: it allows us to do async IO.Assuming that the user is not doing async IO, when a syscall like write() returns, the caller expects the buffer to be written to the disk. Therefore, if we are not using a page cache, we would have to wait for the write to the disk to complete, which can often be extremely slow. By using the page cache, we can give the user the illusion that the buffer was written to the disk via page cache (Which is much faster), and later, we can asynchronously write it back to the disk. Thus, we can return control more quickly to the user program in cases like write() without giving up any functionality, thus making the page cache a very fast and powerful method.However, there are issues with the page cache. Because changes are written back to the disk later, those changes could be sitting in memory, unsynced for long periods. This can cause issues as if a computer crash occurs at an inopportune time, changes could be lost. Because of this, many programs like databases that cannot afford this loss of data use sync() frequently or use the O_DIRECT flag when writing, both of which start writeback immediately.The nature of deferred write is immensely important for Qogchamp and is something that I will get into when discussing mmap.address_spaceAs we discussed in the last post, the main structure used to handle the page cache is struct address_space. Yes this is an awful name as it has nothing to do with an address spaces (That is instead delegated to vm_area_struct), but it’s what we have.Let’s go into more detail into what the structure contains.struct address_space { struct inode *host; struct xarray i_pages; gfp_t gfp_mask; atomic_t i_mmap_writable;#ifdef CONFIG_READ_ONLY_THP_FOR_FS /* number of thp, only for non-shmem files */ atomic_t nr_thps;#endif struct rb_root_cached i_mmap; struct rw_semaphore i_mmap_rwsem; unsigned long nrpages; pgoff_t writeback_index; const struct address_space_operations *a_ops; unsigned long flags; errseq_t wb_err; spinlock_t private_lock; struct list_head private_list; void *private_data;}Above is the struct address_space from the 5.14 Linux Kernel. There are a few important fields.The first field is host. host represents the inode that the page cache is mapping. This can be thought of as the file that the page cache is mapping into memory. I should note that struct inode also references the struct address_space.The next field of importance is i_pages. i_pages is a special kind of array (Don’t ask me how. That’s beyond me) that contains pointers to struct pages. These struct pages are the descriptors for the pages that are the in-memory representation of the file. In other words, this is the meat of the page cache. By kmapping the struct page pointers from i_pages, you will get direct access to the in-memory representation of the file. This is critical to getting the exploit to work.To traverse the struct xarray, there are a suite functions in xarray.h that are available. These functions are used throughout the page cache code.The third field of importance is i_mmap. i_mmap is a rb-interval tree which contains every vm_area_struct that maps this particular inode. For the Linux Kernel, this serves as the “rmap” or reverse mapping for file-backed mappings. In the Linux Kernel, every struct page has a pointer to every pte that maps that particular struct page. In other words, if given a struct page, I can find every pte that maps the contents of that struct page For anonymous pages, it is the anon_vma and for file-backed mappings it is address_space -&amp;gt; i_mmap. We will see this field come up when we discuss mmap.The last field that is important is const struct address_space_operations *a_ops. If you’re familiar with the Linux Kernel, this should be a familiar sight. This struct contains the “member functions” of the address_space object - yes, member functions in the same sense as Java. We will continually revisit this function as we discuss specific syscalls.In order to better demonstrate how struct address_space works with other Linux Kernel data structures, I have placed a fantastic image below.In the picture, the struct page pointers are stored in i_pages and these struct pages are mapped into various VMAs throughout the system. These various VMAs also contain struct files which then point back to the struct address_space.What’s NextFrom here, I want to go into how exactly read, write, mmap, and sync work as the knowledge gained from deeply studying these three syscalls will be critical in understanding how exactly this exploit works.ReferencesInformation on Page Cache - https://courses.cs.vt.edu/~cs5204/fall15-gback/More Information on Page Cache - Understanding the Linux Kernel 3rd Edition by Daniel P. Bovet and Marco Cesati - Chapter 15struct address_space - https://elixir.bootlin.com/linux/latest/source/include/linux/fs.h#L459" }, { "title": "Paging in the Linux Kernel", "url": "/posts/2-paging-in-the-linux-kernel/", "categories": "QogChamp", "tags": "Linux Kernel Exploitation, Paging, MM", "date": "2021-10-29 12:30:00 +0000", "snippet": "Before I begin discussing the internals of the QogChamp exploit, I first need to discuss how the Linux kernel handles paging.This article will assume familiarity with paging as a concept.Quick Refresher on PagingPaging is how we establish a mapping that turns linear or virtual addresses into physical addresses. By using paging, we can do a variety of things such as isolate address spaces as well as prevent external fragmentation of memory.In order to turn a linear address into a physical address, we need to do a page table walk as described below from Intel Manual Volume 3:Note that the directory pointers have 9-bit offsets instead of 10-bit offsets. If you’re used to programming 32-bit paging, then this might take you by surprise. As we will see later, the paging data structures are 8 bytes, so we can only fit 4096/8 = 512 = 2^9 paging data structures in a given directory. Thus, it is sufficient to have a 9 bit offset instead of a 10-bit offset. However, we still need a 12 bit offset into the frame.Paging in the CPUBefore I discuss paging in Linux, it is first important to get a grasp of how paging is handled in the CPU.In order to get an understanding of paging in the CPU, we can reference the trusty Intel Developer Manual Volume 3.Above is a picture of all the paging data structures in IA-32e (64 bit). There are 4 data structures that concern us: the CR3, the PDPTE, the PDE page table, and the 4 KiB page.CR3 is a control register in the CPU that always points to the first PDPTE. This is where the page walk is started. The CR3 is swapped out at every context switch in order to enforce different address spaces.The PDPTE is the first level of paging and contains a 12-bit aligned pointer to a page of PDEs. Typically, we use the first set of 10-bits in the linear address as an offset from the address in CR3 to find the relevant PDPTE.The PDE is the second level of paging and contains a pointer 12-bit aligned pointer to a page of PTEs. Typically, we use the second set of 10 bits in the linear address as an offset from the address in the PDPTE to find the relevant PDE.The third and final level of paging is the PTE. The PTE contains a 12-bit aligned pointer to a page frame. Typically, we use the third set of 10 bits in the linear address as an offset from the address in the PDE to find the relevant PTE.The last 12 bits of a linear address are used to search through the 4 KiB page frame.Another thing that you might notice is that in every paging data structure, the last 12 bits have a bunch of entries. These are known as flags and contain information about the state of the PTE. The one flag that concerns us is the dirty flag (Marked as D). This dirty flag is set to 1 by the hardware whenever the corresponding page frame is written to.Paging in the KernelNow that we have discussed paging as a concept and paging in the CPU, we can finally discuss paging in the Linux Kernel.For the kernel, the PTE data structure does not provide sufficient information about a page. This is because, in the Linux Kernel, pages are associated with various additional structures such as page caches, slabs, and page pools. So, the kernel needs a place to put metadata regarding these structures. This is where struct page comes in. struct page is the Linux Kernel’s own representation of pages. In contrast to the PTE, the struct page is quite large, with each structure being 64 bytes.These page structures contain a variety of extremely important fields, each of which deserves its own lengthy articles, but only a few concern us for the purposes of QogChamp.The first field of importance is:unsigned long flagsThis is an unsigned 64 bit integer that contains the flags of a given page. The purpose of this field is to extend the flag portion of the PTE and provide a kernel developer with a much higher degree of control as well as more options on how to label their pages.The possible flags that can go into unsigned long flags are detailed in page-flags.h. Below I have listed a shortlist of all the possible flags, but if you’re interested in the current flags that a page can have please check it out here.enum pageflags { PG_locked, /* Page is locked. Don&#39;t touch. */ PG_referenced, PG_uptodate, PG_dirty, PG_lru, PG_active, PG_workingset, PG_waiters, /* Page has waiters, check its waitqueue. Must be bit #7 and in the same byte as &quot;PG_locked&quot; */ PG_error, PG_slab, PG_owner_priv_1, /* Owner use. If pagecache, fs may use*/ PG_arch_1, PG_reserved, PG_private, /* If pagecache, has fs-private data */ PG_private_2, /* If pagecache, has fs aux data */ PG_writeback, /* Page is under writeback */ PG_head, /* A head page */ PG_mappedtodisk, /* Has blocks allocated on-disk */ PG_reclaim, /* To be reclaimed asap */ PG_swapbacked, /* Page is backed by RAM/swap */ PG_unevictable /* Page is &quot;unevictable&quot; */};Out of these flags, only a few concern us. The first is PG_uptodate. PG_uptodate signifies that the page’s content is current and matches that on the disk. We will discuss what this means in greater detail as we discuss page and buffer caches.The second is PG_dirty. PG_dirty is a flag that signifies that the page has been written to. It is very important to note that this flag is in no way correlated to the dirty bit on the PTE. The dirty bit on the PTE is set by the CPU automatically when a write occurs. PG_dirty is set by the kernel (Software). We will discuss later when we get into the details of QogChamp about why this distinction is important as well as under what conditions PG_dirty is set.The third bit that is important is PG_private. We will get into this more when we get into the page cache, but what it means is that the private field in the page struct contains a series of struct buffer_heads.The fourth bit that is important is PG_mappedtodisk. All this flag signifies is that the contents of the page correlated to blocks on the disk. We will get into this a lot more as we discuss page cache. This flag is usually when a page is read from disk.The fifth bit is PG_writeback. PG_writeback represents pages that are currently under some disk IO. Pages under writeback should not be modified.The sixth bit of importance is PG_locked. Contrary to what it sounds like, PG_locked does not lock the actual memory that the struct page describes. Rather, it is a lock on the fields in a struct page. Thus, any process wanting to modify a struct page should hold the lock, and this flag should be set.The next field (Or set of fields rather) is the anonymous struct that contains the page cache information:struct { /* Page cache and anonymous pages */ /** * @lru: Pageout list, eg. active_list protected by * lruvec-&amp;gt;lru_lock. Sometimes used as a generic list * by the page owner. struct list_head lru; /* See page-flags.h for PAGE_MAPPING_FLAGS */ struct address_space *mapping; pgoff_t index; /* Our offset within mapping. */ /** * @private: Mapping-private opaque data. * Usually used for buffer_heads if PagePrivate. * Used for swp_entry_t if PageSwapCache. * Indicates order in the buddy system if PageBuddy. */ unsigned long private;};The first field in this anonymous struct is the LRU. The LRU is a very important data structure that deals with page eviction. To my very limited understanding of LRU, there are two lists: active and inactive. Pages in the active list have been accessed very recently, and pages in the inactive list have not been accessed recently. Based on this LRU list, the Linux kernel makes heuristic decisions on whether to flush or keep any page. The LRU list will not be of concern for QogChamp.The second field is struct address_space. This is a particularly interesting field as it can actually be two things. If the page is file-mapped, then the struct address_space field will point to a struct address_space, which contains information on file-mapped shared pages. However, if the page is anonymous (Not file-backed), then it will actually point to a struct anon_vma as evidenced in this function, which is a descendant of the anonymous page fault handler. struct anon_vma is used to keep track of shared anonymous mappings. Only struct address_space will be of concern for QogChamp.The third field is index. The index field is used to describe the page’s index within the mapping field. It works almost in the same way that an index into an array works.The final field is private. The private field is only set if the PG_private flag is set. If PG_private is set, the private field will contain a pointer to the struct buffer_heads. This is something that we will go into when we discuss page caches.The next two fields within the struct page of importance are _mapcount and _refcount. _refcount is exactly what it sounds like - the number of references held to the struct page. Typically, this is increased and decreased via the get_page and put_page functions, respectively._mapcount, on the other hand, is a counter of how many times the page has been mapped to userspace (i.e., how many page tables the page is in). Put simply, _refcount is the number of references held to the struct page, and _mapcount is the number of virtual references held to the page. The page eviction code often uses these two variables to see when a page is safe to evict. These two fields will not be very important, but I felt that I should mention them because they are such ubiquitous variables.What’s nextFrom here, I will try to explain the page cache and how it functions as well as the syscalls that will be important.ReferencesPage walk image - Intel Developer Manual Volume 3A page 4-20Paging data structures image - Intel Developer Manual Volume 3A page 4-18struct page - https://elixir.bootlin.com/linux/latest/source/include/linux/mm_types.h#L70anon_vma - https://lwn.net/Articles/75405/_mapcount and _refcount - https://stackoverflow.com/questions/48931940/how-do-pinned-pages-in-linux-present-or-actually-pin-themselves" }, { "title": "Beginnings", "url": "/posts/1-beginnings/", "categories": "QogChamp", "tags": "Linux Kernel Exploitation, Syscall", "date": "2021-10-26 16:30:00 +0000", "snippet": "In this post, I hope to tell you about how QogChamp started and what the original codebase looked like. This will be a fairly brief post and won’t get too much into the codebase as the functions used in the original codebase are not used anymore in the current implementation of the rootkit.MotivationQogChamp started out in my high school in a class called Computer Systems, in which we completed a capstone project.While searching for an idea for this capstone project, I had the absolutely brilliant idea of attempting to emulate the ptrace() syscall without actually hooking anything. This idea made me go down the rabbit hole that eventually led to QogChamp.Catching SyscallsWhen attempting to emulate ptrace(), I initially started by attempting to catch syscalls. As you might imagine, this is really hard without the faculties of ptrace().The solution I eventually devised ended up using the task_struct regset API. This API is, hilariously, directly a subset of the ptrace API and allows one to get the current registers of any given task_struct. This API was absolutely critical to getting this “exploit” to work.In order to understand how this “exploit” worked, it is important to go over some structures and code. In the original code, I have a function called get_gen_regset. What this function does is that it takes in a pid and returns the struct user_regs_struct (Structure containing the registers and their values) for that pid. This struct user_regs_struct was the focal point of this exploit.Interestingly, struct user_regs_struct has a field called orig_ax, containing the current syscall number (If applicable). This is critical as if orig_ax is a valid syscall number, then we know that a syscall is in progress. Using this fact, we can perform our attack.If orig_ax is a valid syscall number, then we know that the other registers (Specifically di, si, dx, etc.) contain the system call arguments. Through this, we are able to, in a sense, view syscalls as they go back and forth.DrawbacksThis ptrace code was frankly abysmal. It was abysmal for a variety of reasons. First of all, it was completely unpredictable. Due to the nature of this exploit, you were essentially at the mercy of the scheduler/interrupt handler, and you had to hope that your kernel code was running while the program you were trying to track was running.In addition, because the code could never know when a syscall would happen, the code would be forced busy loop (Infinite for loop) until it found a valid syscall number in orig_ax. This, as any seasoned kernel developer will tell you, is a marking of horrible kernel code.QogChamp Begins!It was from this basic exploit that QogChamp got its name and was what inspired me to create the exploit as it is today.QogChamp initially started when I was messing with the write() syscall. Using the technique described above, I was able to get the buffer passed into write(). With a buffer in hand, I wondered what would happen if I modified it. So, I did some research, wrote some page-walk code, and eventually got the capability to modify to the buffer.When I modified the buffer, something astounding happened: the changes were persistent. In other words, if I re-ran the program it would use the modified buffer instead of the original buffer. This odd behavior serves as the inspiration for the rest of the exploit.QogChamp got its name from this because the buffer that I originally testing was &quot;PogChamp&quot; and the change that I made was changing the P to a Q.What’s NextFrom here, I’m going to focus on the exploit as it is, and I’m going to try my best to explain the unbelievably complex clockwork that makes this exploit possible.Referencesget_gen_regset - https://github.com/mineo333/Qogchamp/blob/master/src/regset.c#L18" }, { "title": "What is QogChamp?", "url": "/posts/what-is-qogchamp/", "categories": "QogChamp", "tags": "Linux Kernel Exploitation", "date": "2021-10-26 13:46:00 +0000", "snippet": "So, the primary reason I made this blog was to document and discuss the logic and code behind my rootkit: QogChamp. In this post, I want to answer the simple question: What is QogChamp?Through this post, I hope to answer some very basic questions about its name, its purpose, and other things.What is QogChamp?QogChamp is a novel Linux kernel rootkit built for the 5.14 Linux Kernel that aims to utilize innovative exploits in order to accomplish goals with more power, more effectively, or with a lesser footprint.Why is it called QogChamp?The reason why this exploit is called QogChamp is quite a long story. Basically, what happened is that the very first version of this kernel exploit was created by changing a string &quot;PogChamp&quot; to &quot;QogChamp&quot;. This became a long running joke in my friend group, so the name just stuck.As of today, the codebase and its capabilities have progressed far beyond the original &quot;PogChamp&quot; to &quot;QogChamp&quot;, but the memories and the name remain.What is the purpose of QogChamp?The purpose of QogChamp is to provide a series of novel kernel exploits. In my opinion, today’s Linux rootkits are getting boring as almost all of them fundamentally rely on syscall hooking (Modifying the syscall table) to perform their tasks. Through QogChamp, I hope to provide an alternative to this method of syscall hooking and provide new and exciting ways to do kernel exploitation.What can QogChamp do?QogChamp can do a variety of things. As of the writing of this piece, QogChamp mainly focuses on exploiting the page cache in order to perform file operations with little to no footprint.As of right now, I am working on network capabilities that allow QogChamp to communicate covertly without the use of a socket. The plan is to do this by reading and writing directly from a NIC’s tx and rx ring buffer, but this is very subject to change as I learn more about Linux networking internals.In the future, I hope add more features and turn QogChamp into a proper rootkit.Where can I find QogChamp?You can find it at my GitHub hereIn fact, if you go to the VERY old branch - working_vuln - you will find the original code that changed PogChamp to QogChamp. working-vuln is a complete misnomer and is named as such because originally, I thought this was a legitimate zero-day.What’s next?Over the next few posts, I hope to delve deep into the history of QogChamp as well as explaining what it does and how it gets it done. This entire series will most likely be a combination of story telling as well as technical explanation on the inner workings of QogChamp." } ]
